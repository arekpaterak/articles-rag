{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>1. Introduction of Word2vec\\n\\nWord2vec is one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n",
       "      <td>In my last article, I introduced the concept o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to Use ggplot2 in Python</td>\n",
       "      <td>Introduction\\n\\nThanks to its strict implement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Databricks: How to Save Data Frames as CSV Fil...</td>\n",
       "      <td>Photo credit to Mika Baumeister from Unsplash\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  A Beginner’s Guide to Word Embedding with Gens...   \n",
       "1  Hands-on Graph Neural Networks with PyTorch & ...   \n",
       "2                       How to Use ggplot2 in Python   \n",
       "3  Databricks: How to Save Data Frames as CSV Fil...   \n",
       "4  A Step-by-Step Implementation of Gradient Desc...   \n",
       "\n",
       "                                                Text  \n",
       "0  1. Introduction of Word2vec\\n\\nWord2vec is one...  \n",
       "1  In my last article, I introduced the concept o...  \n",
       "2  Introduction\\n\\nThanks to its strict implement...  \n",
       "3  Photo credit to Mika Baumeister from Unsplash\\...  \n",
       "4  A Step-by-Step Implementation of Gradient Desc...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('articles/medium.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df) == 1391"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles loaded: 1391\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "loader = DataFrameLoader(df, page_content_column=\"Text\")\n",
    "articles = loader.load()\n",
    "\n",
    "print(\"Articles loaded:\", len(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Introduction of Word2vec\n",
      "\n",
      "Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another.\n",
      "\n",
      "There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit. For more details about the word2vec algorithm, please check here.\n",
      "\n",
      "2. Gensim Python Library Introduction\n",
      "\n",
      "Gensim is an open source python library for natural language processing and it was developed and is maintained by the Czech natural language processing researcher Radim Řehůřek. Gensim library will enable us to develop word embeddings by training our own word2vec models on a custom corpus either with CBOW of skip-grams algorithms.\n",
      "\n",
      "At first, we need to install the genism package. Gensim runs on Linux, Windows and Mac OS X, and should run on any other platform that supports Python 2.7+ and NumPy. Gensim depends on the following software:\n",
      "\n",
      "Python >= 2.7 (tested with versions 2.7, 3.5 and 3.6)\n",
      "\n",
      ">= 2.7 (tested with versions 2.7, 3.5 and 3.6) NumPy >= 1.11.3\n",
      "\n",
      ">= 1.11.3 SciPy >= 0.18.1\n",
      "\n",
      ">= 0.18.1 Six >= 1.5.0\n",
      "\n",
      ">= 1.5.0 smart_open >= 1.2.1\n",
      "\n",
      "There are two ways for installation. We could run the following code in our terminal to install genism package.\n",
      "\n",
      "pip install --upgrade gensim\n",
      "\n",
      "Or, alternatively for Conda environments:\n",
      "\n",
      "conda install -c conda-forge gensim\n",
      "\n",
      "3. Implementation of word Embedding with Gensim Word2Vec Model\n",
      "\n",
      "In this tutorial, I will show how to generate word embedding with genism using a concrete example. The dataset I used for this tutorial is from Kaggle Dataset.\n",
      "\n",
      "This vehicle dataset includes features such as make, model, year, engine, and other properties of the car. We will use these features to generate the word embeddings for each make model and then compare the similarities between different make model. The full python tutorial can be found here.\n",
      "\n",
      ">>> df = pd.read_csv('data.csv')\n",
      "\n",
      ">>> df.head()\n",
      "\n",
      "3.1 Data Preprocessing:\n",
      "\n",
      "Since the purpose of this tutorial is to learn how to generate word embeddings using genism library, we will not do the EDA and feature selection for the word2vec model for the sake of simplicity.\n",
      "\n",
      "Genism word2vec requires that a format of ‘list of lists’ for training where every document is contained in a list and every list contains lists of tokens of that document. At first, we need to generate a format of ‘list of lists’ for training the make model word embedding. To be more specific, each make model is contained in a list and every list contains lists of features of that make model.\n",
      "\n",
      "To achieve this, we need to do the following things :\n",
      "\n",
      "a. Create a new column for Make Model\n",
      "\n",
      ">>> df['Maker_Model']= df['Make']+ \" \" + df['Model']\n",
      "\n",
      "b. Generate a format of ‘ list of lists’ for each Make Model with the following features: Engine Fuel Type, Transmission Type, Driven_Wheels, Market Category, Vehicle Size, Vehicle Style.\n",
      "\n",
      "# Select features from original dataset to form a new dataframe\n",
      "\n",
      ">>> df1 = df[['Engine Fuel Type','Transmission Type','Driven_Wheels','Market Category','Vehicle Size', 'Vehicle Style', 'Maker_Model']] # For each row, combine all the columns into one column\n",
      "\n",
      ">>> df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1) # Store them in a pandas dataframe\n",
      "\n",
      ">>> df_clean = pd.DataFrame({'clean': df2}) # Create the list of list format of the custom corpus for gensim modeling\n",
      "\n",
      ">>> sent = [row.split(',') for row in df_clean['clean']] # show the example of list of list format of the custom corpus for gensim modeling\n",
      "\n",
      ">>> sent[:2]\n",
      "\n",
      "[['premium unleaded (required)',\n",
      "\n",
      "'MANUAL',\n",
      "\n",
      "'rear wheel drive',\n",
      "\n",
      "'Factory Tuner',\n",
      "\n",
      "'Luxury',\n",
      "\n",
      "'High-Performance',\n",
      "\n",
      "'Compact',\n",
      "\n",
      "'Coupe',\n",
      "\n",
      "'BMW 1 Series M'],\n",
      "\n",
      "['premium unleaded (required)',\n",
      "\n",
      "'MANUAL',\n",
      "\n",
      "'rear wheel drive',\n",
      "\n",
      "'Luxury',\n",
      "\n",
      "'Performance',\n",
      "\n",
      "'Compact',\n",
      "\n",
      "'Convertible',\n",
      "\n",
      "'BMW 1 Series']]\n",
      "\n",
      "3.2. Genism word2vec Model Training\n",
      "\n",
      "We can train the genism word2vec model with our own custom corpus as following:\n",
      "\n",
      ">>> model = Word2Vec(sent, min_count=1,size= 50,workers=3, window =3, sg = 1)\n",
      "\n",
      "Let’s try to understand the hyperparameters of this model.\n",
      "\n",
      "size: The number of dimensions of the embeddings and the default is 100.\n",
      "\n",
      "window: The maximum distance between a target word and words around the target word. The default window is 5.\n",
      "\n",
      "min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5.\n",
      "\n",
      "workers: The number of partitions during training and the default workers is 3.\n",
      "\n",
      "sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.\n",
      "\n",
      "After training the word2vec model, we can obtain the word embedding directly from the training model as following.\n",
      "\n",
      ">>> model['Toyota Camry'] array([-0.11884457, 0.03035539, -0.0248678 , -0.06297892, -0.01703234,\n",
      "\n",
      "-0.03832747, -0.0825972 , -0.00268112, -0.09192555, -0.08458661,\n",
      "\n",
      "-0.07199778, 0.05235871, 0.21303181, 0.15767808, -0.1883737 ,\n",
      "\n",
      "0.01938575, -0.24431638, 0.04261152, 0.11865819, 0.09881561,\n",
      "\n",
      "-0.04580643, -0.08342388, -0.01355413, -0.07892415, -0.08467747,\n",
      "\n",
      "-0.0040625 , 0.16796461, 0.14578669, 0.04187112, -0.01436194,\n",
      "\n",
      "-0.25554284, 0.25494182, 0.05522631, 0.19295982, 0.14461821,\n",
      "\n",
      "0.14022525, -0.2065216 , -0.05020927, -0.08133671, 0.18031682,\n",
      "\n",
      "0.35042757, 0.0245426 , 0.15938364, -0.05617865, 0.00297452,\n",
      "\n",
      "0.15442047, -0.01286271, 0.13923576, 0.085941 , 0.18811756],\n",
      "\n",
      "dtype=float32)\n",
      "\n",
      "4. Compute Similarities\n",
      "\n",
      "Now we could even use Word2vec to compute the similarity between two Make Models in the vocabulary by invoking the model.similarity( ) and passing in the relevant words. For instance, model.similarity(‘Porsche 718 Cayman’, ‘Nissan Van’) This will give us the Euclidian similarity between Porsche 718 Cayman and Nissan Van.\n",
      "\n",
      ">>> model.similarity('Porsche 718 Cayman', 'Nissan Van')\n",
      "\n",
      "0.822824584626184 >>> model.similarity('Porsche 718 Cayman', 'Mercedes-Benz SLK-Class')\n",
      "\n",
      "0.961089779453727\n",
      "\n",
      "From the above examples, we can tell that Porsche 718 Cayman is more similar to Mercedes-Benz SLK-Class than Nissan Van. We also can use the built-in function model.most_similar() to get a set of the most similar make models for a given make model based on the Euclidean distance.\n",
      "\n",
      ">>> model1.most_similar('Mercedes-Benz SLK-Class')[:5] [('BMW M4', 0.9959905743598938),\n",
      "\n",
      "('Maserati Coupe', 0.9949707984924316),\n",
      "\n",
      "('Porsche Cayman', 0.9945154190063477),\n",
      "\n",
      "('Mercedes-Benz SLS AMG GT', 0.9944609999656677),\n",
      "\n",
      "('Maserati Spyder', 0.9942780137062073)]\n",
      "\n",
      "However, Euclidian similarity cannot work well for the high-dimensional word vectors. This is because Euclidian similarity will increase as the number of dimensions increases, even if the word embedding stands for different meanings. Alternatively, we can use cosine similarity to measure the similarity between two vectors. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity captures the angle of the word vectors and not the magnitude. Under cosine similarity, no similarity is expressed as a 90-degree angle while the total similarity of 1 is at a 0-degree angle.\n",
      "\n",
      "The following function shows how can we generate the most similar make model based on cosine similarity.\n",
      "\n",
      "def cosine_distance (model, word,target_list , num) :\n",
      "\n",
      "cosine_dict ={}\n",
      "\n",
      "word_list = []\n",
      "\n",
      "a = model[word]\n",
      "\n",
      "for item in target_list :\n",
      "\n",
      "if item != word :\n",
      "\n",
      "b = model [item]\n",
      "\n",
      "cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
      "\n",
      "cosine_dict[item] = cos_sim\n",
      "\n",
      "dist_sort=sorted(cosine_dict.items(), key=lambda dist: dist[1],reverse = True) ## in Descedning order\n",
      "\n",
      "for item in dist_sort:\n",
      "\n",
      "word_list.append((item[0], item[1]))\n",
      "\n",
      "return word_list[0:num] # only get the unique Maker_Model\n",
      "\n",
      ">>> Maker_Model = list(df.Maker_Model.unique()) # Show the most similar Mercedes-Benz SLK-Class by cosine distance\n",
      "\n",
      ">>> cosine_distance (model,'Mercedes-Benz SLK-Class',Maker_Model,5) [('Mercedes-Benz CLK-Class', 0.99737006),\n",
      "\n",
      "('Aston Martin DB9', 0.99593246),\n",
      "\n",
      "('Maserati Spyder', 0.99571854),\n",
      "\n",
      "('Ferrari 458 Italia', 0.9952333),\n",
      "\n",
      "('Maserati GranTurismo Convertible', 0.994994)]\n",
      "\n",
      "5. T-SNE Visualizations\n",
      "\n",
      "It’s hard to visualize the word embedding directly, for they usually have more than 3 dimensions. T-SNE is a useful tool to visualize high-dimensional data by dimension reduction while keeping relative pairwise distance between points. It can be said that T-SNE looking for a new data representation where the neighborhood relations are preserved. The following code shows how to plot the word embedding with T-SNE plot.\n",
      "\n",
      "def display_closestwords_tsnescatterplot(model, word, size):\n",
      "\n",
      "\n",
      "\n",
      "arr = np.empty((0,size), dtype='f')\n",
      "\n",
      "word_labels = [word] close_words = model.similar_by_word(word) arr = np.append(arr, np.array([model[word]]), axis=0)\n",
      "\n",
      "for wrd_score in close_words:\n",
      "\n",
      "wrd_vector = model[wrd_score[0]]\n",
      "\n",
      "word_labels.append(wrd_score[0])\n",
      "\n",
      "arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
      "\n",
      "\n",
      "\n",
      "tsne = TSNE(n_components=2, random_state=0)\n",
      "\n",
      "np.set_printoptions(suppress=True)\n",
      "\n",
      "Y = tsne.fit_transform(arr) x_coords = Y[:, 0]\n",
      "\n",
      "y_coords = Y[:, 1]\n",
      "\n",
      "plt.scatter(x_coords, y_coords) for label, x, y in zip(word_labels, x_coords, y_coords):\n",
      "\n",
      "plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
      "\n",
      "plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
      "\n",
      "plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
      "\n",
      "plt.show() >>> display_closestwords_tsnescatterplot(model, 'Porsche 718 Cayman', 50)\n",
      "\n",
      "This T-SNE plot shows the top 10 similar vehicles to the Porsche 718 Cayman in two-dimensional space.\n",
      "\n",
      "About Me\n",
      "\n",
      "I am a master student in Data Science at the University of San Francisco. I am passionate about using Machine Learning to solve business challenges. You can also find me through Linkedin.\n"
     ]
    }
   ],
   "source": [
    "print(articles[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 2731\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # chunk_size=1000,\n",
    "    # chunk_overlap=20,\n",
    "    # length_function=len,\n",
    ")\n",
    "\n",
    "articles = text_splitter.split_documents(articles)\n",
    "print(\"Number of chunks:\", len(articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = articles[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 10/10 [03:28<00:00, 20.81s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(show_progress=True)\n",
    "\n",
    "vector_store = FAISS.from_documents(articles, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved documents: 4\n",
      "Document content: Photo credit to Mika Baumeister from Unsplash\n",
      "\n",
      "When I work on Python projects dealing with large datasets, I usually use Spyder. The environment of Spyder is very simple; I can browse through working directories, maintain large code bases and review data frames I create. However, if I don’t subset the large data, I constantly face memory issues and struggle with very long computational time. For this reason, I occasionally use Databricks. Databricks is a Microsoft Azure platform where you can easily parse large amounts of data into “notebooks” and perform Apache Spark-based analytics.\n",
      "\n",
      "If you want to work with data frames and run models using pyspark, you can easily refer to Databricks’ website for more information. However, while working on Databricks, I noticed that saving files in CSV, which is supposed to be quite easy, is not very straightforward. In the following section, I would like to share how you can save data frames from Databricks into CSV format on your local computer with no hassles.\n",
      "\n",
      "1. Explore the Databricks File System (DBFS)\n",
      "\n",
      "From Azure Databricks home, you can go to “Upload Data” (under Common Tasks)→ “DBFS” → “FileStore”.\n",
      "\n",
      "DBFS FileStore is where you create folders and save your data frames into CSV format. By default, FileStore has three folders: import-stage, plots, and tables.\n",
      "\n",
      "2. Save a data frame into CSV in FileStore\n",
      "\n",
      "Sample.coalesce(1).write.format(“com.databricks.spark.csv”).option(“header”, “true”).save(“dbfs:/FileStore/df/Sample.csv”)\n",
      "\n",
      "Using the above code on the notebook, I created a folder “df” and saved a data frame “Sample” into CSV. It is important to use coalesce(1) since it saves the data frame as a whole. At the end of this article, I will also demonstrate what happens when you don’t include coalesce(1) in the code.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "retrieved = vector_store.similarity_search(\"What is Gensim?\")\n",
    "print(\"Retrieved documents:\", len(retrieved))\n",
    "print(\"Document content:\", retrieved[2].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), config={'run_name': 'format_inputs'})\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], template='\\n   Answer the question based on the provided context:\\n   <context>\\n   {context}\\n   </context>\\n                                          \\n   Question: \\n   {input}\\n'))])\n",
       "| Ollama()\n",
       "| StrOutputParser(), config={'run_name': 'stuff_documents_chain'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama2\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "   Answer the question based on the provided context:\n",
    "   <context>\n",
    "   {context}\n",
    "   </context>\n",
    "                                          \n",
    "   Question: \n",
    "   {input}\n",
    "\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000225408ABFD0>), config={'run_name': 'retrieve_documents'})\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), config={'run_name': 'format_inputs'})\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], template='\\n   Answer the question based on the provided context:\\n   <context>\\n   {context}\\n   </context>\\n                                          \\n   Question: \\n   {input}\\n'))])\n",
       "            | Ollama()\n",
       "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "  }), config={'run_name': 'retrieval_chain'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:06<00:00,  6.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code for implementing gradient descent in Python using a neural network is shown below:\n",
      "```\n",
      "import numpy as np\n",
      "\n",
      "class NeuralNetwork:\n",
      "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
      "        self.input_dim = input_dim\n",
      "        self.hidden_dim = hidden_dim\n",
      "        self.output_dim = output_dim\n",
      "        \n",
      "        self.w = np.random.rand(input_dim, hidden_dim)\n",
      "        self.b = np.zeros((hidden_dim,))\n",
      "        \n",
      "    def sigmoid(self, x):\n",
      "        return 1 / (1 + np.exp(-x))\n",
      "    \n",
      "    def sigmoid_derivative(self, x, w):\n",
      "        return x * (1 - x) * np.exp(x) * (-w)\n",
      "    \n",
      "    def gradient_descent(self, x, y, iterations):\n",
      "        for i in range(iterations):\n",
      "            Xi = x\n",
      "            Xj = self.sigmoid(Xi, self.w)\n",
      "            yhat = self.sigmoid(Xj, self.b)\n",
      "            \n",
      "            # gradients for hidden to output weights\n",
      "            g_b = np.dot(Xj.T, (y - yhat) * self.sigmoid_derivative(Xj, self.b))\n",
      "            \n",
      "            # gradients for input to hidden weights\n",
      "            g_w = np.dot(Xi.T, np.dot((y - yhat) * self.sigmoid_derivative(Xj, self.b), self.w) )\n",
      "            \n",
      "            self.w -= g_w\n",
      "            self.b -= g_b\n",
      "        \n",
      "    def __str__(self):\n",
      "        return \"Neural Network\"\n",
      "```\n",
      "The `gradient_descent` method takes in the input data `x`, the predicted output `y`, and the number of iterations `iterations`. It then performs gradient descent on the weights and biases of the neural network using the sigmoid function. The gradients are calculated for both the hidden to output weights and the input to hidden weights, and the weights and biases are updated in each iteration.\n",
      "\n",
      "To use this code, you will need to define the input data `x` and the predicted output `y`. You can do this using a variety of methods, such as reading data from a file or entering it manually. Once you have defined the input data, you can call the `gradient_descent` method and pass in the number of iterations you want to perform.\n",
      "```\n",
      "# Define the input data\n",
      "x = np.random.rand(100, 3)\n",
      "\n",
      "# Predicted output\n",
      "y = np.sin(x)\n",
      "\n",
      "# Perform gradient descent for 50 iterations\n",
      "self.gradient_descent(x, y, 50)\n",
      "```\n",
      "This code will perform gradient descent on the weights and biases of the neural network using the sigmoid function, starting from a random initial value and updating the weights and biases in each iteration until convergence is reached. The `sigmoid` function is used to compute the sigmoid activation function for the hidden layers, and the `sigmoid_derivative` function is used to compute the derivative of the sigmoid function.\n",
      "\n",
      "It's important to note that this code is just an example and may not work as-is for your specific problem. You may need to modify it to suit your needs, such as changing the activation function or adding regularization terms to prevent overfitting. Additionally, you will need to make sure that the neural network architecture and the optimization algorithm used are appropriate for your problem and data size.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"How to implement the gradient descent?\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
